{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import hgtk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from layer_utils import Attention\n",
    "\n",
    "\n",
    "class Continental(object):\n",
    "    \"\"\"Most functions are defined under this class.\"\"\"\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.path = path\n",
    "        self.equip_name = None\n",
    "        self.top_k = None\n",
    "        self.label2idx = None\n",
    "        self.equip2idx = None\n",
    "        self.num_classes = None\n",
    "        self.num_equip = None\n",
    "        self.num_chars = None\n",
    "        self.max_seq_len = None\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"\n",
    "        Read data, remove unnecessary columns.\n",
    "        Returns data as a pd.DataFrame object.\n",
    "        \"\"\"\n",
    "        # 엑셀 파일로부터 데이터 읽기\n",
    "        data = pd.read_excel(self.path, sheet_name='Sheet1')\n",
    "        # Data column 확인 (불필요시 삭제해도 무방)\n",
    "        print(data.columns)\n",
    "\n",
    "        # 필요한 열 이외 남기고 모두 제거\n",
    "        columns_to_keep = ['Index', '고장원인전처리', '고장조치 라벨링', 'EquiGroup']\n",
    "        data = data.filter(items=columns_to_keep)\n",
    "\n",
    "        # 추후 편의를 위해 column 이름을 교체\n",
    "        data = data.rename(\n",
    "            index=str, columns={'고장원인전처리': 'sentences',\n",
    "                                '고장조치 라벨링': 'labels',\n",
    "                                'EquiGroup': 'equip'}\n",
    "        )\n",
    "        # EquiGroup 빈도 수 확인 (불필요시 삭제해도 무방)\n",
    "        print('equip counts:\\n', data['equip'].value_counts())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def filter_by_equip(self, data, equip_name=None):\n",
    "        \"\"\"\n",
    "        Filter rows by EquiGroup keys.\n",
    "        Unnecessary if all rows must be used.\n",
    "        \"\"\"\n",
    "        self.equip_name = equip_name\n",
    "\n",
    "        if self.equip_name is None:\n",
    "            # 모든 관측치 사용\n",
    "            self.equip_name = 'all'\n",
    "        else:\n",
    "            # EquipGroup 값에 따라 일부 관측치만 사용\n",
    "            equip_mask = data['equip'].isin([self.equip_name])  # 관측치 개수와 동일한 길이의 boolean mask 생성\n",
    "            data = data[equip_mask].copy()                      # Mask 값이 참(True)인 관측치만 선택\n",
    "\n",
    "        # 클래스 레이블의 빈도 수 확인 (불필요시 삭제해도 무방)\n",
    "        print('label counter:\\n', data['labels'].value_counts())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def filter_by_topk_labels(self, data, k):\n",
    "        \"\"\"\n",
    "        Keep rows with top-k labels, based on frequency.\n",
    "        \"\"\"\n",
    "        self.top_k = k\n",
    "        label_counts = data['labels'].value_counts()      # 레이블 빈도 수 확인\n",
    "        labels_to_keep = label_counts.index[:k]           # 가장 많이 등장하는 k개의 레이블 선택\n",
    "        label_mask = data['labels'].isin(labels_to_keep)  # 관측치 개수와 동일한 길이의 boolean mask 생성\n",
    "        data = data[label_mask].copy()                    # Mask 값이 참(True)인 관측치만 선택\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _sent2char(self, x):\n",
    "        \"\"\"Decompose a sentence to characters.\"\"\"\n",
    "        x = hgtk.text.decompose(x)\n",
    "        x = re.sub('\\s+', ' ', x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def insert_columns(self, data):\n",
    "        \"\"\"\n",
    "        Insert columns into the dataframe.\n",
    "        Columns include:\n",
    "            integer labels (y), integer equip info (x_equip), and character-level input (x_char).\n",
    "        \"\"\"\n",
    "\n",
    "        # Add 'y' column (integers, 0 ~ num_classes - 1)\n",
    "        self.label2idx = LabelEncoder()\n",
    "        y = self.label2idx.fit_transform(data['labels'])  # string 타입의 레이블을 정수값으로 변환\n",
    "        data.insert(data.shape[-1], 'y', y)          # dataframe 맨 뒤에 열 추가\n",
    "        self.num_classes = len(data['labels'].unique())\n",
    "\n",
    "        # Add 'x_equip' column (integers, 0 ~ num_equip - 1)\n",
    "        self.equip2idx = LabelEncoder()\n",
    "        x_equip = self.equip2idx.fit_transform(data['equip'])  # string 타입의 equip 정보를 정수값으로 변환\n",
    "        data.insert(data.shape[-1], 'x_equip', x_equip)        # dataframe 맨 뒤에 열 추가\n",
    "        self.num_equip = len(data['x_equip'].unique())\n",
    "\n",
    "        # Add 'X_char' column (characters)\n",
    "        x_char = [self._sent2char(x) for x in data['sentences'].values.tolist()]  # 모든 문장을 character 단위로 변환\n",
    "        data.insert(data.shape[-1], 'x_char', x_char)                             # dataframe 맨 뒤에 열 추가\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_data(self, data):\n",
    "        \"\"\"Save data to pickle file.\"\"\"\n",
    "        assert isinstance(data, pd.DataFrame)\n",
    "        data.to_pickle(\n",
    "            'datasets/subdata_E-{}_C-{}.pkl'.format(self.equip_name, self.top_k)\n",
    "        )\n",
    "\n",
    "    def make_model_inputs(self, subdata):\n",
    "        \"\"\"Additional preprocessing.\"\"\"\n",
    "\n",
    "        # 자소 단위 데이터를 정수 값으로 변환\n",
    "        tokenizer = text.Tokenizer(\n",
    "            num_words=None, filters='', lower=True, split=' ', char_level=True)\n",
    "        tokenizer.fit_on_texts(subdata['x_char'].values)\n",
    "        print('number of characters:', len(tokenizer.index_word))\n",
    "        self.num_chars = len(tokenizer.index_word)\n",
    "\n",
    "        X_char = tokenizer.texts_to_sequences(subdata['x_char'].values)\n",
    "        self.max_seq_len = max([len(x) for x in X_char])\n",
    "        print('max sequence length: ', self.max_seq_len)\n",
    "\n",
    "        # 최대 문장 길이에 맞추기 (짧은 경우 앞에서부터 0으로 padding, 긴 경우 앞에서부터 제거)\n",
    "        X_char = sequence.pad_sequences(X_char, maxlen=self.max_seq_len, truncating='pre', padding='pre')\n",
    "\n",
    "        # Scalar 값으로 표현된 equip 정보를 one-hot vector 형태로 변환\n",
    "        X_equip = keras.utils.to_categorical(subdata['x_equip'].values)\n",
    "\n",
    "        # Scalar 값으로 표현된 y를 one-hot vector 형태로 변환\n",
    "        Y = keras.utils.to_categorical(subdata['y'].values, num_classes=self.num_classes)\n",
    "\n",
    "        return X_char, X_equip, Y\n",
    "\n",
    "    def build_model(self, use_equip_info=False):\n",
    "\n",
    "        # Define character-level input tensor\n",
    "        char_input = Input(shape=(self.max_seq_len,), dtype='int32')\n",
    "\n",
    "        # Integer values to 64-dimensional vectors\n",
    "        h = Embedding(\n",
    "            input_dim=self.num_chars + 1,\n",
    "            output_dim=64,\n",
    "            input_length=self.max_seq_len,\n",
    "            mask_zero=True)(char_input)\n",
    "\n",
    "        # LSTM operation\n",
    "        h = LSTM(\n",
    "            units=128,\n",
    "            return_sequences=True,\n",
    "            unroll=True)(h)\n",
    "\n",
    "        # Attention layer\n",
    "        h = Attention(self.max_seq_len)(h)\n",
    "\n",
    "        if use_equip_info:\n",
    "            equip_input = Input(shape=(self.num_equip, ), dtype='float32')\n",
    "            h_equip = Dense(64, activation='relu')(equip_input)\n",
    "            h = keras.layers.concatenate([h, h_equip])\n",
    "            y = Dense(self.num_classes, activation='softmax')(h)\n",
    "            model = Model([char_input, equip_input], y)\n",
    "        else:\n",
    "            y = Dense(self.num_classes, activation='softmax')(h)\n",
    "            model = Model(char_input, y)\n",
    "\n",
    "        # Define optimizer & compile model\n",
    "        opt = keras.optimizers.Adam()\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=[keras.metrics.categorical_accuracy,\n",
    "                     self.top3_acc,\n",
    "                     self.top5_acc]\n",
    "        )\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def top3_acc(y_true, y_pred):\n",
    "        return keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "    @staticmethod\n",
    "    def top5_acc(y_true, y_pred):\n",
    "        return keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Make data\n",
    "    path = './datasets/PM_fail&solution_data.xlsx'\n",
    "    cont = Continental(path)\n",
    "    top_k = 100\n",
    "    data = cont.read_data()\n",
    "    data = cont.filter_by_equip(data, equip_name=None)\n",
    "    data = cont.filter_by_topk_labels(data, top_k)\n",
    "    data = cont.insert_columns(data)\n",
    "    cont.save_data(data)\n",
    "\n",
    "    # Read data (example)\n",
    "    # data = pd.read_pickle('datasets/subdata_E-all_C-100.pkl')\n",
    "\n",
    "    # Make model inputs\n",
    "    X_char, X_equip, Y = cont.make_model_inputs(subdata=data)\n",
    "\n",
    "    # Instantiate model\n",
    "    model = cont.build_model(use_equip_info=True)  # Equip 정보를 사용하고 싶은 경우에만 True 설정\n",
    "\n",
    "    # Randomly split data into train & test sets\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        np.arange(len(Y)), test_size=0.2, shuffle=True,\n",
    "        random_state=123123\n",
    "    )\n",
    "    X_char_train, X_equip_train, Y_train = X_char[train_indices], X_equip[train_indices], Y[train_indices]\n",
    "    X_char_test, X_equip_test, Y_test = X_char[test_indices], X_equip[test_indices], Y[test_indices]\n",
    "\n",
    "    # Set class weights\n",
    "    cls_wgt = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(np.argmax(Y_train, axis=-1)),\n",
    "        y=np.argmax(Y_train, axis=-1)\n",
    "    )\n",
    "\n",
    "    # Set checkpoint callback\n",
    "    model_path = 'history/model_top-{}.hdf5'.format(top_k)\n",
    "    chkpoint = keras.callbacks.ModelCheckpoint(model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    # Train model\n",
    "    hist = model.fit(\n",
    "        [X_char_train, X_equip_train], Y_train,\n",
    "        epochs=100,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        class_weight=cls_wgt,\n",
    "        verbose=2,\n",
    "        callbacks=[chkpoint]\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    np.save('history/hist_top-{}.npy'.format(top_k), hist.history)\n",
    "\n",
    "    # Load model\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    # Prediction on test set\n",
    "    Y_pred = model.predict([X_char_test, X_equip_test], batch_size=256)\n",
    "    cm = confusion_matrix(np.argmax(Y_test, axis=-1), np.argmax(Y_pred, axis=-1))\n",
    "    print(classification_report(np.argmax(Y_test, axis=-1), np.argmax(Y_pred, axis=-1)))\n",
    "\n",
    "    # Evaluation on test set\n",
    "    test_score = model.evaluate([X_char_test, X_equip_test], Y_test, batch_size=256, verbose=2)\n",
    "    print('test score (top-1, top-3, top5): ({:.3f}, {:.3f}, {:.3f})'.format(*test_score[1:]))\n",
    "\n",
    "    # Save final predictions (top-5)\n",
    "    test_table = []\n",
    "    for test_index, y_pred in zip(test_indices, Y_pred):\n",
    "\n",
    "        # Get top-5 predictions (labels & probabilities)\n",
    "        top_k_indices = np.argsort(y_pred)[::-1][:5]                    # Descending order\n",
    "        top_k_labels = cont.label2idx.inverse_transform(top_k_indices)  # integer label -> original string\n",
    "        top_k_proba = y_pred[top_k_indices]                             # probabilities\n",
    "\n",
    "        # Retrieve original text\n",
    "        sent = data['sentences'][test_index]\n",
    "\n",
    "        # Retrieve original index\n",
    "        original_index = data['Index'][test_index]\n",
    "\n",
    "        # Retrieve true label\n",
    "        true_label = data['labels'][test_index]\n",
    "\n",
    "        # Save values to dictionary\n",
    "        d = {\n",
    "            'text': sent,\n",
    "            'index': original_index,\n",
    "            'true': true_label\n",
    "        }\n",
    "        for i, (_, lb, pb) in enumerate(zip(top_k_indices, top_k_labels, top_k_proba)):\n",
    "            d.update(\n",
    "                {'top-{}-pred'.format(i + 1): lb,\n",
    "                 'top-{}-prob'.format(i + 1): pb}\n",
    "            )\n",
    "\n",
    "        test_table.append(d)\n",
    "\n",
    "    test_table = pd.DataFrame(test_table)\n",
    "\n",
    "    # top 100\n",
    "    # loss, top1 acc, top3 acc, top5 acc\n",
    "    # [1.9646599202466826, 0.4786316862890516, 0.7061740984076353, 0.7994808566197856]\n",
    "    '''\n",
    "                 precision    recall  f1-score   support\n",
    "          0       0.73      0.72      0.72        71\n",
    "          1       0.46      0.53      0.50      2161\n",
    "          2       0.29      0.18      0.22        68\n",
    "          3       0.40      0.10      0.16        61\n",
    "          4       0.00      0.00      0.00        24\n",
    "          5       0.20      0.07      0.10       247\n",
    "          6       0.00      0.00      0.00        14\n",
    "          7       0.19      0.14      0.16        21\n",
    "          8       0.40      0.12      0.18        51\n",
    "          9       0.00      0.00      0.00        26\n",
    "         10       0.66      0.58      0.62       122\n",
    "         11       0.57      0.08      0.15        48\n",
    "         12       0.47      0.32      0.38        56\n",
    "         13       0.73      0.22      0.33        37\n",
    "         14       0.42      0.57      0.48        40\n",
    "         15       0.65      0.57      0.61       633\n",
    "         16       0.27      0.02      0.04       132\n",
    "         17       0.16      0.10      0.12        29\n",
    "         18       0.77      0.83      0.80       869\n",
    "         19       0.29      0.42      0.34       353\n",
    "         20       0.33      0.07      0.12        56\n",
    "         21       0.00      0.00      0.00        30\n",
    "         22       0.00      0.00      0.00        25\n",
    "         23       0.00      0.00      0.00        60\n",
    "         24       0.00      0.00      0.00        29\n",
    "         25       0.17      0.03      0.05        34\n",
    "         26       0.00      0.00      0.00        38\n",
    "         27       0.00      0.00      0.00        21\n",
    "         28       0.08      0.01      0.02       101\n",
    "         29       0.50      0.03      0.06        61\n",
    "         30       0.17      0.02      0.03        55\n",
    "         31       0.67      0.11      0.18        38\n",
    "         32       0.62      0.14      0.22        37\n",
    "         33       0.64      0.27      0.38        67\n",
    "         34       0.41      0.06      0.10       425\n",
    "         35       0.00      0.00      0.00        36\n",
    "         36       0.33      0.21      0.26        19\n",
    "         37       0.34      0.28      0.30       156\n",
    "         38       0.83      0.64      0.72        94\n",
    "         39       0.38      0.15      0.22        33\n",
    "         40       0.29      0.13      0.18       374\n",
    "         41       0.33      0.37      0.35       796\n",
    "         42       0.31      0.26      0.28       350\n",
    "         43       0.42      0.45      0.44        49\n",
    "         44       0.51      0.73      0.60      2621\n",
    "         45       0.51      0.66      0.58       441\n",
    "         46       0.35      0.21      0.26        53\n",
    "         47       0.27      0.37      0.32        75\n",
    "         48       0.40      0.08      0.13        26\n",
    "         49       0.33      0.04      0.07        27\n",
    "         50       0.29      0.06      0.09       181\n",
    "         51       0.10      0.05      0.07       105\n",
    "         52       0.35      0.25      0.29        24\n",
    "         53       0.90      0.11      0.20        81\n",
    "         54       0.22      0.11      0.14        19\n",
    "         55       0.33      0.04      0.07        25\n",
    "         56       0.38      0.58      0.46        86\n",
    "         57       0.22      0.14      0.17        43\n",
    "         58       0.45      0.35      0.39        84\n",
    "         59       0.47      0.47      0.47        19\n",
    "         60       0.32      0.16      0.21        63\n",
    "         61       0.58      0.49      0.53        89\n",
    "         62       0.00      0.00      0.00        31\n",
    "         63       0.00      0.00      0.00        31\n",
    "         64       0.50      0.70      0.59       251\n",
    "         65       0.11      0.03      0.05        33\n",
    "         66       0.00      0.00      0.00        26\n",
    "         67       0.00      0.00      0.00        27\n",
    "         68       0.54      0.63      0.58      2186\n",
    "         69       0.26      0.13      0.17       123\n",
    "         70       0.45      0.44      0.44      2558\n",
    "         71       0.00      0.00      0.00        89\n",
    "         72       0.33      0.03      0.06        33\n",
    "         73       0.51      0.49      0.50       162\n",
    "         74       0.27      0.13      0.18       252\n",
    "         75       0.00      0.00      0.00        17\n",
    "         76       0.00      0.00      0.00        24\n",
    "         77       0.00      0.00      0.00        29\n",
    "         78       0.00      0.00      0.00        28\n",
    "         79       0.27      0.06      0.10        65\n",
    "         80       0.00      0.00      0.00        26\n",
    "         81       0.12      0.06      0.08        93\n",
    "         82       0.00      0.00      0.00        30\n",
    "         83       0.50      0.07      0.12        90\n",
    "         84       0.21      0.15      0.18        52\n",
    "         85       0.00      0.00      0.00        40\n",
    "         86       0.00      0.00      0.00        32\n",
    "         87       0.30      0.20      0.24       150\n",
    "         88       0.41      0.73      0.53      1134\n",
    "         89       0.33      0.05      0.09        79\n",
    "         90       0.52      0.28      0.36        50\n",
    "         91       0.70      0.83      0.76       748\n",
    "         92       0.50      0.77      0.61        93\n",
    "         93       0.30      0.24      0.26       268\n",
    "         94       0.23      0.25      0.24        51\n",
    "         95       0.36      0.42      0.39       443\n",
    "         96       0.18      0.13      0.15        15\n",
    "         97       0.00      0.00      0.00        29\n",
    "         98       0.25      0.03      0.05        40\n",
    "         99       0.00      0.00      0.00        37\n",
    "avg / total       0.45      0.48      0.44     21574\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
